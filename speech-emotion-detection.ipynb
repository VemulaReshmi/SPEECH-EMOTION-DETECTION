{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MLP.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SKNo9RTBj1hA"
      },
      "source": [
        "#Speech Emotion Recognition with MLP Classifier\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AztUA74EerMQ"
      },
      "source": [
        "#Dataset\n",
        "The Ryerson Audio-Visual Database of Emotional Speech and Song (RAVDESS) \n",
        "\n",
        "---\n",
        "Audio-only files\n",
        "\n",
        "Audio-only files of all actors (01-24) are available as two separate zip files (~200 MB each):\n",
        "\n",
        "Speech file (Audio_Speech_Actors_01-24.zip, 215 MB) contains 1440 files: 60 trials per actor x 24 actors = 1440. \n",
        "Song file (Audio_Song_Actors_01-24.zip, 198 MB) contains 1012 files: 44 trials per actor x 23 actors = 1012.\n",
        "\n",
        "Total=2452\n",
        "\n",
        "---\n",
        "\n",
        "---\n",
        "Toronto emotional speech set (TESS)\n",
        "\n",
        "---\n",
        "\n",
        "\n",
        "There are a set of 200 target words were spoken in the carrier phrase \"Say the word _' by two actresses (aged 26 and 64 years) and recordings were made of the set portraying each of seven emotions (anger, disgust, fear, happiness, pleasant surprise, sadness, and neutral). There are 2800 data points (audio files) in total.\n",
        "\n",
        "The dataset is organised such that each of the two female actor and their emotions are contain within its own folder. And within that, all 200 target words audio file can be found. The format of the audio file is a WAV format\n",
        "\n",
        "\n",
        "---\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ix0uuwXsXIQ6"
      },
      "source": [
        "# Mount google drive\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tjUtB3Elrudq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "897bf63f-ade6-4541-aabb-68927f3f4fec"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nrH14Ttlo2EL"
      },
      "source": [
        "# Install following libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "spdyH1iXo5lB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "db5b699d-eff8-4c1b-b412-9304d49788d9"
      },
      "source": [
        "!pip install librosa soundfile numpy sklearn pyaudio"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: librosa in /usr/local/lib/python3.7/dist-packages (0.8.1)\n",
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n",
            "Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n",
            "Collecting pyaudio\n",
            "  Downloading PyAudio-0.2.11.tar.gz (37 kB)\n",
            "Requirement already satisfied: decorator>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (21.0)\n",
            "Requirement already satisfied: scikit-learn!=0.19.0,>=0.14.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.22.2.post1)\n",
            "Requirement already satisfied: numba>=0.43.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.51.2)\n",
            "Requirement already satisfied: resampy>=0.2.2 in /usr/local/lib/python3.7/dist-packages (from librosa) (0.2.2)\n",
            "Requirement already satisfied: joblib>=0.14 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.0.1)\n",
            "Requirement already satisfied: scipy>=1.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.4.1)\n",
            "Requirement already satisfied: audioread>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (2.1.9)\n",
            "Requirement already satisfied: pooch>=1.0 in /usr/local/lib/python3.7/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.20)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba>=0.43.0->librosa) (0.34.0)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->librosa) (2.4.7)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (1.4.4)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from pooch>=1.0->librosa) (2.23.0)\n",
            "Requirement already satisfied: six>=1.3 in /usr/local/lib/python3.7/dist-packages (from resampy>=0.2.2->librosa) (1.15.0)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->pooch>=1.0->librosa) (2021.5.30)\n",
            "Building wheels for collected packages: pyaudio\n",
            "  Building wheel for pyaudio (setup.py) ... \u001b[?25lerror\n",
            "\u001b[31m  ERROR: Failed building wheel for pyaudio\u001b[0m\n",
            "\u001b[?25h  Running setup.py clean for pyaudio\n",
            "Failed to build pyaudio\n",
            "Installing collected packages: pyaudio\n",
            "    Running setup.py install for pyaudio ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[31mERROR: Command errored out with exit status 1: /usr/bin/python3 -u -c 'import io, os, sys, setuptools, tokenize; sys.argv[0] = '\"'\"'/tmp/pip-install-bp0lsy1r/pyaudio_cc0252fb246140ea8042fdeb1e4a0e82/setup.py'\"'\"'; __file__='\"'\"'/tmp/pip-install-bp0lsy1r/pyaudio_cc0252fb246140ea8042fdeb1e4a0e82/setup.py'\"'\"';f = getattr(tokenize, '\"'\"'open'\"'\"', open)(__file__) if os.path.exists(__file__) else io.StringIO('\"'\"'from setuptools import setup; setup()'\"'\"');code = f.read().replace('\"'\"'\\r\\n'\"'\"', '\"'\"'\\n'\"'\"');f.close();exec(compile(code, __file__, '\"'\"'exec'\"'\"'))' install --record /tmp/pip-record-gilx5_15/install-record.txt --single-version-externally-managed --compile --install-headers /usr/local/include/python3.7/pyaudio Check the logs for full command output.\u001b[0m\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JdmYeo1-fXtz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4cbef344-3c2a-461d-c88e-ea6a1ffa454f"
      },
      "source": [
        "!pip install soundfile"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: soundfile in /usr/local/lib/python3.7/dist-packages (0.10.3.post1)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.7/dist-packages (from soundfile) (1.14.6)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.7/dist-packages (from cffi>=1.0->soundfile) (2.20)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tqlY-_LUpD3l"
      },
      "source": [
        "# Make the necessary imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3fyYSbcJjy7i"
      },
      "source": [
        "import librosa\n",
        "import soundfile\n",
        "import os, glob, pickle\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.metrics import accuracy_score"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6QufSKr1ksJc"
      },
      "source": [
        "Define a function extract_feature to extract the mfcc, chroma, and mel features from a sound file. This function takes 4 parameters- the file name and three Boolean parameters for the three features:\n",
        "\n",
        "* mfcc: Mel Frequency Cepstral Coefficient, represents the short-term power spectrum of a sound\n",
        "* chroma: Pertains to the 12 different pitch classes\n",
        "* mel: Mel Spectrogram Frequency"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dwB6D_vLpOoR"
      },
      "source": [
        "def extract_feature(file_name, mfcc, chroma, mel):\n",
        "    X, sample_rate = librosa.load(os.path.join(file_name), res_type='kaiser_fast')\n",
        "    if chroma:\n",
        "        stft=np.abs(librosa.stft(X))\n",
        "    result=np.array([])\n",
        "    if mfcc:\n",
        "        mfccs=np.mean(librosa.feature.mfcc(y=X, sr=sample_rate, n_mfcc=40).T, axis=0)\n",
        "        result=np.hstack((result, mfccs))\n",
        "    if chroma:\n",
        "        chroma=np.mean(librosa.feature.chroma_stft(S=stft, sr=sample_rate).T,axis=0)\n",
        "        result=np.hstack((result, chroma))\n",
        "    if mel:\n",
        "        mel=np.mean(librosa.feature.melspectrogram(X, sr=sample_rate).T,axis=0)\n",
        "        result=np.hstack((result, mel))\n",
        "    return result"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6xKzPyVcjvyT"
      },
      "source": [
        "Now, let’s define a dictionary to hold numbers and the emotions available in the RAVDESS & TESS dataset, and a list to hold all 8 emotions- neutral,calm,happy,sad,angry,fearful,disgust,surprised."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EgwcKBQzpdgM"
      },
      "source": [
        "# Emotions in the RAVDESS & TESS dataset\n",
        "emotions={\n",
        "  '01':'neutral',\n",
        "  '02':'calm',\n",
        "  '03':'happy',\n",
        "  '04':'sad',\n",
        "  '05':'angry',\n",
        "  '06':'fearful',\n",
        "  '07':'disgust',\n",
        "  '08':'surprised'\n",
        "}\n",
        "# Emotions to observe\n",
        "observed_emotions=['neutral','calm','happy','sad','angry','fearful', 'disgust','surprised']"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VjDiYuC3G-D"
      },
      "source": [
        "# Load the data and extract features for each sound file"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f2axyBFDqI1A"
      },
      "source": [
        "\n",
        "def load_data(test_size=0.2):\n",
        "    x,y=[],[]\n",
        "    for file in glob.glob('/content/drive/MyDrive/speech-emotion-recognition-ravdess-data/Actor_*/*.wav'):\n",
        "        file_name=os.path.basename(file)\n",
        "        emotion=emotions[file_name.split(\"-\")[2]]\n",
        "        if emotion not in observed_emotions:\n",
        "            continue\n",
        "        feature=extract_feature(file, mfcc=True, chroma=True, mel=True)\n",
        "        x.append(feature)\n",
        "        y.append(emotion)\n",
        "    return train_test_split(np.array(x), y, test_size=test_size, train_size= 0.75,random_state=9)"
      ],
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wKhGTlqpqOAr"
      },
      "source": [
        "# Split the Dataset\n",
        "Time to split the dataset into training and testing sets! Let’s keep the test set 25% of everything and use the load_data function for this."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a92bN5vAqRb2"
      },
      "source": [
        "# Split the dataset\n",
        "import time\n",
        "x_train,x_test,y_train,y_test=load_data(test_size=0.25)"
      ],
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "46eeiQbYqT7p"
      },
      "source": [
        "#Observe the shape of the training and testing datasets:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_MXFh4lwqYaC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c1b8717-ef3b-482e-bb20-271212f7424e"
      },
      "source": [
        "#Get the shape of the training and testing datasets\n",
        "print((x_train.shape[0], x_test.shape[0]))"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1080, 360)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmwRPZe7qey5"
      },
      "source": [
        "# Number of features extracted."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H3C6czeDqgxX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "35fabb45-a6ed-471c-d776-706c6e61878f"
      },
      "source": [
        "# Get the number of features extracted\n",
        "print(f'Features extracted: {x_train.shape[1]}')"
      ],
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Features extracted: 180\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YEvGnx7r3W7f"
      },
      "source": [
        "# MLP Classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pvggD94ZqmAY"
      },
      "source": [
        "# Initialize the Multi Layer Perceptron Classifier\n",
        "model=MLPClassifier(alpha=0.01, batch_size=256, epsilon=1e-08, hidden_layer_sizes=(300,), learning_rate='adaptive', max_iter=500)"
      ],
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cG4YLLXGqpAb"
      },
      "source": [
        "#Fit/train the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9OfJEVsDqrbY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "51c9b63f-7656-4c95-da1b-9d62b7859389"
      },
      "source": [
        "# Train the model\n",
        "model.fit(x_train,y_train)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MLPClassifier(activation='relu', alpha=0.01, batch_size=256, beta_1=0.9,\n",
              "              beta_2=0.999, early_stopping=False, epsilon=1e-08,\n",
              "              hidden_layer_sizes=(300,), learning_rate='adaptive',\n",
              "              learning_rate_init=0.001, max_fun=15000, max_iter=500,\n",
              "              momentum=0.9, n_iter_no_change=10, nesterovs_momentum=True,\n",
              "              power_t=0.5, random_state=None, shuffle=True, solver='adam',\n",
              "              tol=0.0001, validation_fraction=0.1, verbose=False,\n",
              "              warm_start=False)"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iDC7AkAt21EF"
      },
      "source": [
        "# Predict the accuracy of our model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D2Kzw2Emqt1p"
      },
      "source": [
        "Let’s predict the values for the test set. This gives us y_pred (the predicted emotions for the features in the test set)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNVNDVnBqw1z"
      },
      "source": [
        "# Predict for the test set\n",
        "y_pred=model.predict(x_test)"
      ],
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "njU6Gs6jqzkg"
      },
      "source": [
        "To calculate the accuracy of our model, we’ll call up the accuracy_score() function we imported from sklearn. Finally, we’ll round the accuracy to 2 decimal places and print it out."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_sDEv4K1q1VA",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b726d9d7-d23a-4e34-913d-8a7054ed57e4"
      },
      "source": [
        "# Calculate the accuracy of our model\n",
        "accuracy=accuracy_score(y_true=y_test, y_pred=y_pred)\n",
        "# Print the accuracy\n",
        "print(\"Accuracy: {:.2f}%\".format(accuracy*100))"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 40.00%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGdh6fh02hFc"
      },
      "source": [
        "#classification Report"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2xreI8SSq6Rc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "380ead9f-d114-4370-af02-d8aa7835c34e"
      },
      "source": [
        "from sklearn.metrics import classification_report\n",
        "print(classification_report(y_test,y_pred))\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "       angry       0.79      0.53      0.64        64\n",
            "        calm       0.42      0.26      0.32        43\n",
            "     disgust       0.20      0.68      0.31        31\n",
            "     fearful       0.80      0.24      0.38        49\n",
            "       happy       0.53      0.45      0.49        44\n",
            "     neutral       0.00      0.00      0.00        34\n",
            "         sad       0.30      0.70      0.42        46\n",
            "   surprised       0.56      0.29      0.38        49\n",
            "\n",
            "    accuracy                           0.40       360\n",
            "   macro avg       0.45      0.39      0.37       360\n",
            "weighted avg       0.50      0.40      0.39       360\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KZZZBmK32ksS"
      },
      "source": [
        "# Confusion Matrix"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Y2IEjtiTtl-f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b31a48f8-4220-4bfd-dd91-737a991cd9c3"
      },
      "source": [
        "from sklearn.metrics import confusion_matrix\n",
        "matrix = confusion_matrix(y_test,y_pred)\n",
        "print (matrix)"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[34  1 22  0  4  0  1  2]\n",
            " [ 1 11 16  0  0  1 14  0]\n",
            " [ 0  0 21  1  3  0  5  1]\n",
            " [ 4  0  7 12  4  0 18  4]\n",
            " [ 3  0  8  2 20  0  9  2]\n",
            " [ 0  7  7  0  1  0 18  1]\n",
            " [ 0  6  4  0  1  2 32  1]\n",
            " [ 1  1 20  0  5  0  8 14]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EbipGM_Xnfyb"
      },
      "source": [
        "#Thank You"
      ]
    }
  ]
}